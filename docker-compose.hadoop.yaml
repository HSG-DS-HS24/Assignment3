services:

  namenode:
    image: galess/hadoop:3
    hostname: namenode
    container_name: namenode
    # See https://stackoverflow.com/a/72109503
    user: root
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    command: ["hdfs", "namenode"]
    # The image does not have the starter script at build time
    entrypoint: ["/opt/starter.sh"]
    environment:
      - ENSURE_NAMENODE_DIR=/tmp/hadoop-root/dfs/name
      - SPARK_USER=root
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - ./hadoop-config-files/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hadoop-config-files/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./hadoop-config-files/yarn-site.xml:/opt/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop-config-files/mapred-site.xml:/opt/hadoop/etc/hadoop/mapred-site.xml
      - ./hadoop-config-files/hadoop-env.sh:/opt/hadoop/etc/hadoop/hadoop-env.sh
      - ./docker/starter.sh:/opt/starter.sh
    networks:
      sparknet:
        ipv4_address: 172.23.0.2


  datanode:
    image: galess/hadoop:3
    depends_on:
      - namenode
    hostname: datanode
    container_name: datanode
    user: root
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    # The image does not have the starter script at build time
    # The starter script for this node contains steps specific to the datanode
    entrypoint: ["/opt/starter-new.sh"]
    environment:
        - ENSURE_INPUT_DIR=/input
        - INPUT_FILE_DIR=/root/input
        - SPARK_USER=root
    ports:
      - 9864:9864
      - 9866:9866
    volumes:
      - ./input:/root/input
      - ./tasks/app/build/libs/app.jar:/root/app.jar
      - ./hadoop-config-files/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hadoop-config-files/yarn-site.xml:/opt/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop-config-files/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./hadoop-config-files/mapred-site.xml:/opt/hadoop/etc/hadoop/mapred-site.xml
      - ./hadoop-config-files/hadoop-env.sh:/opt/hadoop/etc/hadoop/hadoop-env.sh
      - ./docker/starter.sh:/opt/starter.sh
      - ./docker/starter-ext.sh:/opt/starter-ext.sh
      - ./docker/starter-new.sh:/opt/starter-new.sh
    networks:
      sparknet:
        ipv4_address: 172.23.0.3

  # dps:
  #   image: defreitas/dns-proxy-server
  #   ports:
  #     - 5380:5380
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock
  #     - /etc/resolv.conf:/etc/resolv.conf
  #   hostname: dns.mageddo
  #   container_name: dps
  #   depends_on:
  #     - namenode
  #   networks:
  #     sparknet:
  #       ipv4_address: 172.23.0.20

networks:
  sparknet:
    name: A3
    external: true
